<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-06-15 Tue 14:56 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Natural Language Processing</title>
<meta name="author" content="Isitha Subasinghe" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Natural Language Processing</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1bde88c">1. Text Processing</a>
<ul>
<li><a href="#orge3db40e">1.1. Words</a></li>
<li><a href="#orgc680cc7">1.2. Sentence</a></li>
<li><a href="#org8c2b039">1.3. Document</a></li>
<li><a href="#orga2bbfe4">1.4. Corpus</a></li>
<li><a href="#orgd499649">1.5. Word Token</a></li>
<li><a href="#org9142409">1.6. Word type</a></li>
<li><a href="#org18266e3">1.7. Why preprocess?</a></li>
<li><a href="#org2ccd06b">1.8. Preprocessing steps:</a></li>
<li><a href="#org6d5fb01">1.9. Sentence segmentation</a>
<ul>
<li><a href="#orgae3f05e">1.9.1. Binary Classifier</a></li>
</ul>
</li>
<li><a href="#org04a4512">1.10. Word Tokenisation</a></li>
<li><a href="#orgf704c63">1.11. Subword tokenisation</a></li>
<li><a href="#orgde35f65">1.12. Word Normalisation</a></li>
<li><a href="#org7f786e2">1.13. Inflectional Morphology</a></li>
<li><a href="#orgeb2f555">1.14. Lemmatisation</a></li>
<li><a href="#orga2e8722">1.15. Derivation morphology</a></li>
<li><a href="#org2ff16b9">1.16. Stemming</a></li>
<li><a href="#org2c82de0">1.17. Porter Stemmer</a></li>
<li><a href="#org02edbe1">1.18. Fix Spelling Errors</a></li>
<li><a href="#orgda2f115">1.19. Other word normalisation</a></li>
<li><a href="#orgc5cad49">1.20. Stopword Removal</a></li>
</ul>
</li>
<li><a href="#org9fe77be">2. N-gram Language Models</a>
<ul>
<li><a href="#org785b300">2.1. Probabilities: Joint to Conditional</a>
<ul>
<li><a href="#org1717d3f">2.1.1. The Markov assumption</a></li>
<li><a href="#org03f3848">2.1.2. Maximum Likelihood Estimation</a></li>
<li><a href="#org468ffe9">2.1.3. Book ending sentences</a></li>
<li><a href="#orgfc1f39f">2.1.4. Several problems</a></li>
<li><a href="#orgd4f6d8c">2.1.5. Smoothing</a></li>
<li><a href="#orgc95e836">2.1.6. Generation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf7d52f3">3. Text Classification</a>
<ul>
<li><a href="#org583db06">3.1. Building a Text Classifier</a></li>
<li><a href="#org6de8fa9">3.2. Choosing a classification algorithm</a></li>
<li><a href="#orgbb0c410">3.3. Naive Bayes</a></li>
<li><a href="#org2329a26">3.4. Logistic Regression</a></li>
<li><a href="#orgc1e0b8d">3.5. Support Vector Machines</a></li>
<li><a href="#org5dce309">3.6. K-Nearest Neighbour</a></li>
<li><a href="#org025d40e">3.7. Decision Tree</a></li>
<li><a href="#org35be03f">3.8. Random forests</a></li>
<li><a href="#org86b9c59">3.9. Neural Networks</a></li>
<li><a href="#org434c39d">3.10. Evaluation</a>
<ul>
<li><a href="#org7a73d8e">3.10.1. Accuracy</a></li>
<li><a href="#org24f67d8">3.10.2. Precision &amp; Recall</a></li>
<li><a href="#org1d3610d">3.10.3. F(1) score</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org31df1b0">4. Part of Speech Tagging</a>
<ul>
<li><a href="#org3ff1f9d">4.1. Information Extraction</a></li>
<li><a href="#org6e255de">4.2. POS Open Classes</a></li>
<li><a href="#org1541390">4.3. POS Closed Classes</a></li>
<li><a href="#orgc07b051">4.4. Ambiguity</a></li>
<li><a href="#org1976270">4.5. Tagsets</a>
<ul>
<li><a href="#orgdd7376e">4.5.1. Major Penn Treebank Tags</a></li>
</ul>
</li>
<li><a href="#orgb855598">4.6. Derived Tags (Open Class)</a></li>
<li><a href="#org84a80dc">4.7. Derived Tags (Closed Class)</a></li>
<li><a href="#orgc120776">4.8. Automatic Taggers</a>
<ul>
<li><a href="#orgb584034">4.8.1. Rule based taggers</a></li>
<li><a href="#orgc9280f4">4.8.2. Unigram taggers</a></li>
<li><a href="#orge9279a9">4.8.3. Classifier based tagging</a></li>
<li><a href="#org32d1b64">4.8.4. Hidden Markov Models</a></li>
</ul>
</li>
<li><a href="#org2423cb7">4.9. Unknown Words</a></li>
</ul>
</li>
<li><a href="#org1dfdd7c">5. Sequence Tagging: Hidden Markov Models</a>
<ul>
<li><a href="#org9cd3360">5.1. A probabilistic model</a></li>
<li><a href="#org4c27818">5.2. The Viterbi Algorithm</a></li>
</ul>
</li>
<li><a href="#orgccf0065">6. Deep Learning Feedforward Networks</a></li>
<li><a href="#org45b0dc8">7. Deep Learning Recurrent Networks</a></li>
<li><a href="#orgd329d59">8. Lexical Semantics</a></li>
<li><a href="#org8197e19">9. Distributional Semantics</a></li>
<li><a href="#orgab6eef0">10. Question Answering</a>
<ul>
<li><a href="#org8ac9121">10.1. 2 key approaches</a></li>
<li><a href="#org3bf9bc1">10.2. IR based QA</a>
<ul>
<li><a href="#orgf67bb82">10.2.1. IR-based Factoid QA: TREC-QA</a></li>
</ul>
</li>
<li><a href="#orgf10c493">10.3. Feature-Based Answer Extraction</a></li>
<li><a href="#orgfc981ed">10.4. Neural Answer Extraction</a></li>
</ul>
</li>
<li><a href="#org76a94b4">11. Topic Modelling</a></li>
</ul>
</div>
</div>
<div id="outline-container-org1bde88c" class="outline-2">
<h2 id="org1bde88c"><span class="section-number-2">1</span> Text Processing</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orge3db40e" class="outline-3">
<h3 id="orge3db40e"><span class="section-number-3">1.1</span> Words</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Sequence of characters with meaning and/or function
</p>
</div>
</div>
<div id="outline-container-orgc680cc7" class="outline-3">
<h3 id="orgc680cc7"><span class="section-number-3">1.2</span> Sentence</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Sequence of words with meaning and/or function
</p>
</div>
</div>
<div id="outline-container-org8c2b039" class="outline-3">
<h3 id="org8c2b039"><span class="section-number-3">1.3</span> Document</h3>
<div class="outline-text-3" id="text-1-3">
<p>
One or more sentences
</p>
</div>
</div>
<div id="outline-container-orga2bbfe4" class="outline-3">
<h3 id="orga2bbfe4"><span class="section-number-3">1.4</span> Corpus</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Collection of documents
</p>
</div>
</div>
<div id="outline-container-orgd499649" class="outline-3">
<h3 id="orgd499649"><span class="section-number-3">1.5</span> Word Token</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Each instance of a word
</p>
</div>
</div>
<div id="outline-container-org9142409" class="outline-3">
<h3 id="org9142409"><span class="section-number-3">1.6</span> Word type</h3>
<div class="outline-text-3" id="text-1-6">
<p>
Distinct words
</p>
</div>
</div>
<div id="outline-container-org18266e3" class="outline-3">
<h3 id="org18266e3"><span class="section-number-3">1.7</span> Why preprocess?</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>Most NLP applications have documents as inputs</li>
<li><b><b>key point:</b></b> Language is <b><b>compositional.</b></b> As humans, we can break these documents
into individual components. To understand language, a computer should do the same.</li>
<li><b><b>Preprocessing</b></b> is the first step</li>
</ul>
</div>
</div>
<div id="outline-container-org2ccd06b" class="outline-3">
<h3 id="org2ccd06b"><span class="section-number-3">1.8</span> Preprocessing steps:</h3>
<div class="outline-text-3" id="text-1-8">
<ol class="org-ol">
<li>Remove unwanted formatting</li>
<li><b><b>Sentence segmentation:</b></b> break documents into sentences</li>
<li><b><b>Word tokenisation:</b></b> break sentences into words</li>
<li><b><b>Word normalisation:</b></b> transform words into canonical forms</li>
<li><b><b>Stopword removal:</b></b> remove unwanted words</li>
</ol>
</div>
</div>

<div id="outline-container-org6d5fb01" class="outline-3">
<h3 id="org6d5fb01"><span class="section-number-3">1.9</span> Sentence segmentation</h3>
<div class="outline-text-3" id="text-1-9">
<ul class="org-ul">
<li>Naive approach: break on punctuation</li>
<li>Use Regex to capture capital</li>
<li>Have lexicons</li>
<li>State of the art uses machine learning</li>
</ul>
</div>
<div id="outline-container-orgae3f05e" class="outline-4">
<h4 id="orgae3f05e"><span class="section-number-4">1.9.1</span> Binary Classifier</h4>
<div class="outline-text-4" id="text-1-9-1">
<ul class="org-ul">
<li>Looks at every &rsquo;.&rsquo; and decides if it is the end of a sentence.</li>
<li>Features: word shapes(uppercase, lowercase, numbers, ALL<sub>CAPS</sub>), words before and after &rsquo;.&rsquo;, part of speech tags</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org04a4512" class="outline-3">
<h3 id="org04a4512"><span class="section-number-3">1.10</span> Word Tokenisation</h3>
<div class="outline-text-3" id="text-1-10">
<ul class="org-ul">
<li>Naive approach: seperate out alphanumeric strings (\w+)</li>
<li>Some asian languages are written without spaces, we need MaxMatch algorithm here where we greedily match longest word in vocab</li>
<li>Some languages such as German require compound splitter</li>
</ul>
</div>
</div>

<div id="outline-container-orgf704c63" class="outline-3">
<h3 id="orgf704c63"><span class="section-number-3">1.11</span> Subword tokenisation</h3>
<div class="outline-text-3" id="text-1-11">
<ul class="org-ul">
<li>Colourless green ideas -&gt; [colour] [less] [green] [idea] [s]</li>
<li>One popular algorithm: byte-pair encoding (BPE)</li>
<li>Core idea: iteratively merge frequent pair of characters</li>
<li>Advantage:
<ul class="org-ul">
<li>Data-informed tokenisation</li>
<li>Works for different languages</li>
<li>Deals better with unknown words</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgde35f65" class="outline-3">
<h3 id="orgde35f65"><span class="section-number-3">1.12</span> Word Normalisation</h3>
<div class="outline-text-3" id="text-1-12">
<ul class="org-ul">
<li>Lower casing</li>
<li>Removing morphology (cooking -&gt; cook)</li>
<li>Correcting spelling</li>
<li>Expanding Abbreviations</li>
<li>Goal:
<ul class="org-ul">
<li>Reduce vocabulary</li>
<li>Maps words into the same type</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7f786e2" class="outline-3">
<h3 id="org7f786e2"><span class="section-number-3">1.13</span> Inflectional Morphology</h3>
<div class="outline-text-3" id="text-1-13">
<ul class="org-ul">
<li>Inflectional morphology creates grammatical variants</li>
<li>English inflects nouns, verbs and adjectives</li>
<li>Many languages have much richer inflectional morphology</li>
</ul>
</div>
</div>
<div id="outline-container-orgeb2f555" class="outline-3">
<h3 id="orgeb2f555"><span class="section-number-3">1.14</span> Lemmatisation</h3>
<div class="outline-text-3" id="text-1-14">
<ul class="org-ul">
<li>Lemmatisation means removing any inflection to reach the uninflected form, the lemma</li>
<li>In English, there are irregularities that prevent a trivial solution:
<ul class="org-ul">
<li>poked -&gt; poke</li>
<li>stopping -&gt; stop</li>
<li>watches -&gt; watch</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga2e8722" class="outline-3">
<h3 id="orga2e8722"><span class="section-number-3">1.15</span> Derivation morphology</h3>
<div class="outline-text-3" id="text-1-15">
<ul class="org-ul">
<li>Derivation morphology creates distinct words</li>
<li>English derivational suffixes often change the lexical category</li>
<li>English derivational prefixes often change the meaning without changing the lexical category
<ul class="org-ul">
<li>write -&gt; rewrite</li>
<li>healthy -&gt; unhealthy</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2ff16b9" class="outline-3">
<h3 id="org2ff16b9"><span class="section-number-3">1.16</span> Stemming</h3>
<div class="outline-text-3" id="text-1-16">
<ul class="org-ul">
<li>Stemming strips off all suffixes, leaving a stem</li>
<li>automate, automatic -&gt; automat</li>
<li>Even less lexical sparsity than lemmatisation</li>
<li>Popular in information retrieval</li>
<li>Stem not always interpretable</li>
</ul>
</div>
</div>
<div id="outline-container-org2c82de0" class="outline-3">
<h3 id="org2c82de0"><span class="section-number-3">1.17</span> Porter Stemmer</h3>
<div class="outline-text-3" id="text-1-17">
<ul class="org-ul">
<li><p>
Word has three forms
</p>
<ul class="org-ul">
<li>CVCV&#x2026;C</li>
<li>CVCV&#x2026;V</li>
<li>VCVC&#x2026;C</li>
<li>VCVC&#x2026;V</li>
</ul>
<p>
Which can be represented as:
</p>
<ul class="org-ul">
<li>[C]VCVC&#x2026;[V]</li>
<li>[C](VC)<sup>m</sup>[V]</li>
<li>m = <b><b>measure</b></b></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org02edbe1" class="outline-3">
<h3 id="org02edbe1"><span class="section-number-3">1.18</span> Fix Spelling Errors</h3>
<div class="outline-text-3" id="text-1-18">
<ul class="org-ul">
<li>Why fix them?
<ul class="org-ul">
<li>Spelling errors create new, rare types</li>
<li>Disrupt various kinds of analysis</li>
<li>Very common in internet corpus</li>
<li>In web search, particularly important in queries</li>
</ul></li>
<li>How?
<ul class="org-ul">
<li>String distance</li>
<li>Modelling of error types (phonetic, typing, etc)</li>
<li>Use an n-gram language model</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgda2f115" class="outline-3">
<h3 id="orgda2f115"><span class="section-number-3">1.19</span> Other word normalisation</h3>
<div class="outline-text-3" id="text-1-19">
<ul class="org-ul">
<li>Normalise spelling variations</li>
<li>Expanding abbreviations</li>
</ul>
</div>
</div>

<div id="outline-container-orgc5cad49" class="outline-3">
<h3 id="orgc5cad49"><span class="section-number-3">1.20</span> Stopword Removal</h3>
<div class="outline-text-3" id="text-1-20">
<ul class="org-ul">
<li>Definition: a list of words to be removed from the document
<ul class="org-ul">
<li>Typical in BOW representations</li>
</ul></li>
<li>How to chose them?
<ul class="org-ul">
<li>All closed-class or function words</li>
<li>Any high frequency words</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9fe77be" class="outline-2">
<h2 id="org9fe77be"><span class="section-number-2">2</span> N-gram Language Models</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>One application NLP is about explaining language.</li>
<li>We can measure &ldquo;goodness&rdquo; using <b><b>probabilities</b></b></li>
<li>Language models can also be used for <b><b>generation</b></b></li>
<li>Useful for
<ul class="org-ul">
<li>Query completion</li>
<li>Optical character recognition</li>
<li>Other generation tasks
<ul class="org-ul">
<li>Machine translation, summarisation, dialogue systems</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org785b300" class="outline-3">
<h3 id="org785b300"><span class="section-number-3">2.1</span> Probabilities: Joint to Conditional</h3>
<div class="outline-text-3" id="text-2-1">
<p>
\(P(w_1, w_2, w_3, ..., w_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_1,w_2)...P(w_m|w_1,...,w_{m-1})\)
</p>
</div>
<div id="outline-container-org1717d3f" class="outline-4">
<h4 id="org1717d3f"><span class="section-number-4">2.1.1</span> The Markov assumption</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
\(P(w_i|w_1,...w_{i-1}) \approx P(w_i|w_{i-n+1}...w_{i-1})\)
For some small n
</p>

<p>
When n = 1, a unigram model:
</p>

<p>
\(P(w_1,w_2,...w_m) = \prod_{i=1}^{m} P(w_i)\)
</p>

<p>
When n = 2, a bigram model:
</p>

<p>
\(P(w_1, w_2,...w_m) = \prod_{i=1}^{m} P(w_i|w_{i-1})\)
</p>

<p>
When n = 3, a trigram model:
</p>

<p>
\(P(w_1,w_2,...w_m) = \prod_{i=1}^{m} P(w_i|w_{i-2} w_{i-1})\)
</p>
</div>
</div>

<div id="outline-container-org03f3848" class="outline-4">
<h4 id="org03f3848"><span class="section-number-4">2.1.2</span> Maximum Likelihood Estimation</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
How do we calculate the probabilities? Estimate based on counts in our corpus
</p>

<p>
For unigram models:
</p>

<p>
\(P(w_i) = \frac{C(w_i)}{M}\)
</p>

<p>
For bigram models:
</p>

<p>
\(P(w_i|w_{i-1}) = \frac{C(w_{i-1}w_{i})}{C(w_{i-1})}\)
</p>

<p>
For n gram models more generally:
</p>

<p>
\(P(w_i| w_{i-n+1}...w_{i-1}) = \frac{C(w_{i-n+1}...w_i)}{C(w_{i-n+1}...w_{i-1})}\)
</p>
</div>
</div>

<div id="outline-container-org468ffe9" class="outline-4">
<h4 id="org468ffe9"><span class="section-number-4">2.1.3</span> Book ending sentences</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li>Special tokens used to indicate start and end of sequences.
&lt;s&gt; = start of sentence
&lt;/s&gt; = end of sentence</li>
</ul>
</div>
</div>


<div id="outline-container-orgfc1f39f" class="outline-4">
<h4 id="orgfc1f39f"><span class="section-number-4">2.1.4</span> Several problems</h4>
<div class="outline-text-4" id="text-2-1-4">
<ul class="org-ul">
<li>Language has long distance effects - need large n</li>
<li>Resulting probabilities are too small
<ul class="org-ul">
<li>Use log probability</li>
</ul></li>
<li>What about unseen words?
Special symbol</li>
<li>Unseen n-grams
<ul class="org-ul">
<li>Need to smooth the language model</li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-orgd4f6d8c" class="outline-4">
<h4 id="orgd4f6d8c"><span class="section-number-4">2.1.5</span> Smoothing</h4>
<div class="outline-text-4" id="text-2-1-5">
<ul class="org-ul">
<li>Basic idea: give events you haven&rsquo;t seen before some probability</li>
<li>Must be the case that P(everything) = 1</li>
<li>Many different kinds of smoothing
<ul class="org-ul">
<li>Laplacian (add-one) smoothing</li>
<li>Add-k smoothing</li>
<li>Absolute discounting</li>
<li>Kneser-Ney</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc95e836" class="outline-4">
<h4 id="orgc95e836"><span class="section-number-4">2.1.6</span> Generation</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
Given an intial word, draw the next word according to the probability distribution produced by the language model.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf7d52f3" class="outline-2">
<h2 id="orgf7d52f3"><span class="section-number-2">3</span> Text Classification</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Input:
<ul class="org-ul">
<li>A document d</li>
<li>A fixed output set of classes \(C\)</li>
</ul></li>
<li>Output:
<ul class="org-ul">
<li>A predicted class \(c \in C\)</li>
</ul></li>
<li>Text Classification Tasks
<ul class="org-ul">
<li>Topic Classification</li>
<li>Sentiment analysis</li>
<li>Native language identification</li>
<li>Natural language inference</li>
<li>Automatic fact-checking</li>
<li>Paraphrase</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org583db06" class="outline-3">
<h3 id="org583db06"><span class="section-number-3">3.1</span> Building a Text Classifier</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>Identify a task of interest</li>
<li>Collect an appropriate corpus</li>
<li>Carry out annotation</li>
<li>Select features</li>
<li>Choose machine learning algorithm</li>
<li>Train model and tune hyperparameters using held-out development data</li>
<li>Repeat earlier steps as needed</li>
<li>Train final model</li>
<li>Evaluate final model on held-out test data</li>
</ul>
</div>
</div>

<div id="outline-container-org6de8fa9" class="outline-3">
<h3 id="org6de8fa9"><span class="section-number-3">3.2</span> Choosing a classification algorithm</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>Bias vs Variance
<ul class="org-ul">
<li>Bias: assumptions we made in our model</li>
<li>Variance: sensitivity to training set</li>
</ul></li>
<li>Underlying assumptions, e.g. independance</li>
<li>Complexity</li>
<li>Speed</li>
</ul>
</div>
</div>

<div id="outline-container-orgbb0c410" class="outline-3">
<h3 id="orgbb0c410"><span class="section-number-3">3.3</span> Naive Bayes</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Finds the class with highest likelihood under Bayes law
</p>
<ul class="org-ul">
<li>Naively assumes features are independent</li>
<li>Pros:
<ul class="org-ul">
<li>Fast to train and classify</li>
<li>Robust, low variance</li>
<li>optimal classifier if independence assumption is correct</li>
<li>extremely simple to implement</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Independence assumption rarely holds</li>
<li>Low accuracy compared to similar methods in most situations</li>
<li>Smoothing required for unseen class/feature combinations</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2329a26" class="outline-3">
<h3 id="org2329a26"><span class="section-number-3">3.4</span> Logistic Regression</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>A classifer despite its name</li>
<li>A linear model, but uses softmax &ldquo;squashing&rdquo; to get valid probabilty</li>
<li>Pros:
<ul class="org-ul">
<li>Not confounded by diverse correlated features -&gt; better performance</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Slow to train</li>
<li>Feature scaling needed</li>
<li>Requires a lot of data to work well in practice</li>
<li>Choosing regularisation strategy is important since overfitting is a big problem</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc1e0b8d" class="outline-3">
<h3 id="orgc1e0b8d"><span class="section-number-3">3.5</span> Support Vector Machines</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Finds a hyperplane which seperates the training data with maximum margin
</p>
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Fast to train</li>
<li>Can do non-linearity with kernel trick</li>
<li>Works well with huge feature sets</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Multiclass awkward</li>
<li>Feature scaling needed</li>
<li>Deals poorly with class imbalances</li>
<li>Interpretability</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5dce309" class="outline-3">
<h3 id="org5dce309"><span class="section-number-3">3.6</span> K-Nearest Neighbour</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Classify based on majority class of k-nearest neighbours
</p>
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>simple but effective</li>
<li>No training required</li>
<li>Inherently multiclass</li>
<li>Optimal classifer with infinite data</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Have to select k</li>
<li>Issues with imbalanced classes</li>
<li>Often slow</li>
<li>Features must be selected carefully</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org025d40e" class="outline-3">
<h3 id="org025d40e"><span class="section-number-3">3.7</span> Decision Tree</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Fast to build and test</li>
<li>Feature scaling irrelevant</li>
<li>Good for small feature sets</li>
<li>Handles non-linearly seperable problems</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>In practice not that interpretable</li>
<li>highly redundant subtrees</li>
<li>Not competitive for large feature sets</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org35be03f" class="outline-3">
<h3 id="org35be03f"><span class="section-number-3">3.8</span> Random forests</h3>
<div class="outline-text-3" id="text-3-8">
<p>
Consists of a forest of decision trees trained on subsets of training and feature spaces.
Majority vote determines class
</p>
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>More accurate and more robust than decision trees</li>
<li>Great classifier for medium feature sets</li>
<li>Training easily parallelised</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Intepretability</li>
<li>Slow with large feature sets</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org86b9c59" class="outline-3">
<h3 id="org86b9c59"><span class="section-number-3">3.9</span> Neural Networks</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Extrmely powerful</li>
<li>Little feature engineering</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Not off the shelf</li>
<li>Many hyper params, difficult to optimise</li>
<li>Slow to train</li>
<li>Prone to overfitting</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org434c39d" class="outline-3">
<h3 id="org434c39d"><span class="section-number-3">3.10</span> Evaluation</h3>
<div class="outline-text-3" id="text-3-10">
</div>
<div id="outline-container-org7a73d8e" class="outline-4">
<h4 id="org7a73d8e"><span class="section-number-4">3.10.1</span> Accuracy</h4>
<div class="outline-text-4" id="text-3-10-1">
<p>
correct classifications/ total classifications
</p>
</div>
</div>
<div id="outline-container-org24f67d8" class="outline-4">
<h4 id="org24f67d8"><span class="section-number-4">3.10.2</span> Precision &amp; Recall</h4>
<div class="outline-text-4" id="text-3-10-2">
<p>
Hold one class as &ldquo;positive class&rdquo;
Precision = correct classifications of B (TP) / total classifications of B (TP + FP)
Recall = correct classifications of B (TP) / total instances of B (TP + FN)
</p>
</div>
</div>
<div id="outline-container-org1d3610d" class="outline-4">
<h4 id="org1d3610d"><span class="section-number-4">3.10.3</span> F(1) score</h4>
<div class="outline-text-4" id="text-3-10-3">
<ul class="org-ul">
<li>Harmonic mean of precision and recall</li>
</ul>
<p>
\(F1 = \frac{2*precision*recall}{precision+recall}\)
</p>
<ul class="org-ul">
<li>Can be used as a multiclass metric
<ul class="org-ul">
<li>Macroaveraged: Average F1 score across classes</li>
<li>Microaveraged: Calculate F1 score using sum of counts</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org31df1b0" class="outline-2">
<h2 id="org31df1b0"><span class="section-number-2">4</span> Part of Speech Tagging</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>AKA word classes, morphological classes, syntatic categories</li>
<li>Nouns, verbs, adjectives, etc.</li>
<li>POS tells us quite a bit about a word and its neighbours
<ul class="org-ul">
<li>Nouns are often preceded by determiners</li>
<li>Verbs preceded by nouns</li>
<li>Content as a <b><b>noun</b></b> pronounced as CONtent</li>
<li>Content as an <b><b>adjective</b></b> pronounced as conTENT</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org3ff1f9d" class="outline-3">
<h3 id="org3ff1f9d"><span class="section-number-3">4.1</span> Information Extraction</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Given this &ldquo;Brasila, the Brazilian capital, was founded in 1960.&rdquo;</li>
<li>Obtain this:
<ul class="org-ul">
<li>capital(Brazil, Brasilia)</li>
<li>founded(Brasilia, 1960)</li>
</ul></li>
<li>Many steps involved but first need to know <b><b>nouns</b></b>, <b><b>adjectives</b></b>, <b><b>verbs</b></b> and <b><b>numbers</b></b></li>
</ul>
</div>
</div>
<div id="outline-container-org6e255de" class="outline-3">
<h3 id="org6e255de"><span class="section-number-3">4.2</span> POS Open Classes</h3>
<div class="outline-text-3" id="text-4-2">
<p>
<b><b>Open</b></b> vs <b><b>Closed</b></b> classes: how readily do POS categories take on new words? Just a few open classes:
</p>
<ul class="org-ul">
<li>Nouns:
<ul class="org-ul">
<li>Proper (Australia) vs Common (Wombat)</li>
<li>Mass (rice) vs count (bowls)</li>
</ul></li>
<li>Verbs:
<ul class="org-ul">
<li>Rich inflection (go/goes/going/gone/went)</li>
<li>Auxiliary words (be, have, do)</li>
<li>Transitivity</li>
</ul></li>
<li>Adjectives:
<ul class="org-ul">
<li>Gradable (happy) vs non-gradeable (computational)</li>
</ul></li>
<li>Adverbs:
<ul class="org-ul">
<li>Manner (slowly)</li>
<li>Locative (here)</li>
<li>Degree (really)</li>
<li>Temporal (today)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1541390" class="outline-3">
<h3 id="org1541390"><span class="section-number-3">4.3</span> POS Closed Classes</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>Prepositions (in, on, with, for, of, over)
<ul class="org-ul">
<li><b><b>on</b></b> the table</li>
</ul></li>
<li>Particles
<ul class="org-ul">
<li>brushed himself <b><b>off</b></b></li>
</ul></li>
<li>Determiners
<ul class="org-ul">
<li>Articles (a, an, the)</li>
<li>Demonstratives (this, that, these, those)</li>
<li>Quantifiers (each, every, some, two)</li>
</ul></li>
<li>Pronouns
<ul class="org-ul">
<li>Personal (I, me, she)</li>
<li>Possessive (my, our)</li>
<li>Interrogative or Wh (who, what)</li>
</ul></li>
<li>Conjunction
<ul class="org-ul">
<li>Coordinating (and, or, but)</li>
<li>Subordinating (if, although, that)</li>
</ul></li>
<li>Modal verbs:
<ul class="org-ul">
<li>Ability (can, could)</li>
<li>Persmission (can, may)</li>
<li>Possibility (may, might, could will)</li>
<li>Necessity (must)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc07b051" class="outline-3">
<h3 id="orgc07b051"><span class="section-number-3">4.4</span> Ambiguity</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Many word types belong to multiple classes</li>
<li>POS depends on context</li>
<li>Compare:
<ul class="org-ul">
<li>Time flies like an arrow</li>
<li>Fruit flies like a banana</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org1976270" class="outline-3">
<h3 id="org1976270"><span class="section-number-3">4.5</span> Tagsets</h3>
<div class="outline-text-3" id="text-4-5">
<p>
A compact representation of POS information
</p>
<ul class="org-ul">
<li>Usually &lt;= 4 capatilized characters (e.g. NN = noun)</li>
</ul>
</div>
<div id="outline-container-orgdd7376e" class="outline-4">
<h4 id="orgdd7376e"><span class="section-number-4">4.5.1</span> Major Penn Treebank Tags</h4>
<div class="outline-text-4" id="text-4-5-1">
<ul class="org-ul">
<li>NN = noun</li>
<li>JJ = adjective</li>
<li>VB = verb</li>
<li>RB = adverb</li>
<li>DT = determiner</li>
<li>IN = preposition</li>
<li>MD = modal</li>
<li>RP = particle</li>
<li>TO = to</li>
<li>CD = cardinal number</li>
<li>PRP = personal pronoun</li>
<li>CC = coordinating conjuction</li>
<li>WH = wh-pronoun</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgb855598" class="outline-3">
<h3 id="orgb855598"><span class="section-number-3">4.6</span> Derived Tags (Open Class)</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>NN
<ul class="org-ul">
<li>NNS (plural, wombats)</li>
<li>NNP (proper, Australia)</li>
<li>NNPS (proper plural, Australians)</li>
</ul></li>
</ul>
<ul class="org-ul">
<li>VB(verb infinitive, eat)
<ul class="org-ul">
<li>VBP (1st/2nd person present, eat)</li>
<li>VBZ (3rd person singular, eats)</li>
<li>VBD (past tense, ate)</li>
<li>VBG (gerund, eating)</li>
<li>VBN (past participle, eaten)</li>
</ul></li>
<li>JJ (adjective, nice)
<ul class="org-ul">
<li>JJR(comparative, nicer)</li>
<li>JJS(superlative, nicest)</li>
</ul></li>
<li>RB(adverb, fast)
<ul class="org-ul">
<li>RBR(comparitive, faster)</li>
<li>RBS(superlative, fastest)</li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-org84a80dc" class="outline-3">
<h3 id="org84a80dc"><span class="section-number-3">4.7</span> Derived Tags (Closed Class)</h3>
<div class="outline-text-3" id="text-4-7">
<ul class="org-ul">
<li>PRP (pronouns personal, I)
<ul class="org-ul">
<li>PRP$ (possessive, my)</li>
</ul></li>
<li>WP(Wh-pronoun, what)
<ul class="org-ul">
<li>WP$(possessive, whose)</li>
<li>WDT(wh-determiner,which)</li>
<li>WRB(wh-adverb,where)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc120776" class="outline-3">
<h3 id="orgc120776"><span class="section-number-3">4.8</span> Automatic Taggers</h3>
<div class="outline-text-3" id="text-4-8">
<ul class="org-ul">
<li>Rule based taggers</li>
<li>Statistical taggers
<ul class="org-ul">
<li>Unigram tagger</li>
<li>Classifier based taggers</li>
<li>Hidden Markov Model (HMM) taggers</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgb584034" class="outline-4">
<h4 id="orgb584034"><span class="section-number-4">4.8.1</span> Rule based taggers</h4>
<div class="outline-text-4" id="text-4-8-1">
<ul class="org-ul">
<li>Typically starts with a list of possible tags for each word</li>
<li>Often includes other lexical information, e.g. verb subcategorisation</li>
<li>Apply rules to narrow down to a single tag</li>
<li>Large systems have 1000s of constraints</li>
</ul>
</div>
</div>
<div id="outline-container-orgc9280f4" class="outline-4">
<h4 id="orgc9280f4"><span class="section-number-4">4.8.2</span> Unigram taggers</h4>
<div class="outline-text-4" id="text-4-8-2">
<ul class="org-ul">
<li>Assign most common tag to each word type</li>
<li>Requires a corpus of tagged words</li>
<li>&ldquo;Model&rdquo; is just a look up table</li>
<li>But actually quite good, ~90% accuracy</li>
<li>Often considered the baseline</li>
</ul>
</div>
</div>
<div id="outline-container-orge9279a9" class="outline-4">
<h4 id="orge9279a9"><span class="section-number-4">4.8.3</span> Classifier based tagging</h4>
<div class="outline-text-4" id="text-4-8-3">
<ul class="org-ul">
<li>Use a standard discriminative classifier with features</li>
<li>But can suffer from error propagation: wrong predictions from previous steps affect the next ones</li>
</ul>
</div>
</div>

<div id="outline-container-org32d1b64" class="outline-4">
<h4 id="org32d1b64"><span class="section-number-4">4.8.4</span> Hidden Markov Models</h4>
<div class="outline-text-4" id="text-4-8-4">
<ul class="org-ul">
<li>A basic (or structured) model</li>
<li>Like sequential classifiers, use both previous tag and lexical evidence</li>
<li>Unlike classifiers, considers all possibilities of previous tag</li>
<li>Unlike classifiers, treat previous tag evidence and lexical evidence as independent from each other</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org2423cb7" class="outline-3">
<h3 id="org2423cb7"><span class="section-number-3">4.9</span> Unknown Words</h3>
<div class="outline-text-3" id="text-4-9">
<ul class="org-ul">
<li>Huge problem in morphologically rich languages (Turkish)</li>
<li>Can use things we&rsquo;ve seen only once to best guess for things we&rsquo;ve never seen before</li>
<li>Can use subword representations to capture morphology</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1dfdd7c" class="outline-2">
<h2 id="org1dfdd7c"><span class="section-number-2">5</span> Sequence Tagging: Hidden Markov Models</h2>
<div class="outline-text-2" id="text-5">
<p>
Tagging is a sentence level task but as humans we decompose it into small word level tasks
Solution: define a model that decomposes process into individual word level steps
</p>
<ul class="org-ul">
<li>This is the idea of <b><b>sequence labelling</b></b>, and more general, <b><b>structured prediction</b></b>.</li>
</ul>
</div>
<div id="outline-container-org9cd3360" class="outline-3">
<h3 id="org9cd3360"><span class="section-number-3">5.1</span> A probabilistic model</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>\(\hat{t} = argmax_t P(t|w)\)</li>
<li>\(\hat{t} = argmax_t \frac{P(w|t)P(t)}{P(w)}\)</li>
<li>Lets decompose:
<ul class="org-ul">
<li>\(P(w|t) = \prod_{i=1}^{n} P(w_i|t_i)\)</li>
<li>\(P(w) = \prod_{i=1}^{n} P(t_i, t_{i-1})\)</li>
</ul></li>
<li>HMMs - Training
<ul class="org-ul">
<li>Parameters are the individual probabilities
<ul class="org-ul">
<li>\(P(w_i|t_i)\) = emission (O) probabilities</li>
<li>\(P(t_i|t_{i-1})\) = transition (A) probabilities</li>
</ul></li>
<li>Training uses Maximum Likelihood Estimation
This is done by simply counting word frequencies according to their tags (just like n-gram LMs)
<ul class="org-ul">
<li>\(P(like|VB) = \frac{count(VB, like)}{count(VB)}\)</li>
<li>\(P(NN|DT) = \frac{count(DT, NN)}{count(DT)}\)</li>
<li>What about the first tag? Assume we have a symbol &ldquo;STARTSYMBOL&rdquo;</li>
<li>What about unseen (word, tag) and (tag, previous<sub>tag</sub>) combos?
<ul class="org-ul">
<li>\(P(NN | STARTSYMBOL) = \frac{count(STARTSYMBOL, NN)}{count(STARTSYMBOL)}\)</li>
<li>Smoothing techniques</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4c27818" class="outline-3">
<h3 id="org4c27818"><span class="section-number-3">5.2</span> The Viterbi Algorithm</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>Complexity \(O(T^2 * N)\), where T is the size of the tagset and N is the length of the sequence. T*N matrix</li>
<li>Why does it work? Because of the <b><b>independence</b></b> assumption that decomposes the problem.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgccf0065" class="outline-2">
<h2 id="orgccf0065"><span class="section-number-2">6</span> Deep Learning Feedforward Networks</h2>
</div>
<div id="outline-container-org45b0dc8" class="outline-2">
<h2 id="org45b0dc8"><span class="section-number-2">7</span> Deep Learning Recurrent Networks</h2>
</div>
<div id="outline-container-orgd329d59" class="outline-2">
<h2 id="orgd329d59"><span class="section-number-2">8</span> Lexical Semantics</h2>
</div>
<div id="outline-container-org8197e19" class="outline-2">
<h2 id="org8197e19"><span class="section-number-2">9</span> Distributional Semantics</h2>
</div>
<div id="outline-container-orgab6eef0" class="outline-2">
<h2 id="orgab6eef0"><span class="section-number-2">10</span> Question Answering</h2>
<div class="outline-text-2" id="text-10">
<p>
<b>Definition:</b> question answering (&ldquo;QA&rdquo;) is the task of automatically determinging the answer for a natural language question.
</p>
</div>
<div id="outline-container-org8ac9121" class="outline-3">
<h3 id="org8ac9121"><span class="section-number-3">10.1</span> 2 key approaches</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>Information Retrieval based QA
<ul class="org-ul">
<li>Given a query, search relevant documents.</li>
<li>Find answers within these relevant documents.</li>
</ul></li>
<li>Knowledge based QA
<ul class="org-ul">
<li>Builds semantic representation of the query.</li>
<li>Query database of facts to find answers.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3bf9bc1" class="outline-3">
<h3 id="org3bf9bc1"><span class="section-number-3">10.2</span> IR based QA</h3>
<div class="outline-text-3" id="text-10-2">
</div>
<div id="outline-container-orgf67bb82" class="outline-4">
<h4 id="orgf67bb82"><span class="section-number-4">10.2.1</span> IR-based Factoid QA: TREC-QA</h4>
<div class="outline-text-4" id="text-10-2-1">
<ol class="org-ol">
<li>Use question to make a query for the IR engine.</li>
<li>Find document, and passage within document</li>
<li>Extract short answer string</li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org1cdde3c"></a>Question Processing<br />
<div class="outline-text-5" id="text-10-2-1-1">
<ul class="org-ul">
<li>Find key parts of question that will help retrieval
<ul class="org-ul">
<li>discard structural parts (wh-word, ?, etc)</li>
<li>forumlate as tf-idf query, using unigrams or bigrams</li>
<li>identify entities and prioritise match</li>
</ul></li>
<li>May reformulate question using templates
<ul class="org-ul">
<li>e.g. &ldquo;Where is Federation Square located?&rdquo;
<ul class="org-ul">
<li>query = &ldquo;Federation Square located&rdquo;</li>
</ul></li>
</ul></li>
<li>Predict expected answer type (here = LOCATION)</li>
</ul>
</div>
</li>
<li><a id="orgec55327"></a>Answer Types<br />
<div class="outline-text-5" id="text-10-2-1-2">
<ul class="org-ul">
<li>Find a concise answer to the question, as a span in the text.</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgf10c493" class="outline-3">
<h3 id="orgf10c493"><span class="section-number-3">10.3</span> Feature-Based Answer Extraction</h3>
<div class="outline-text-3" id="text-10-3">
<ul class="org-ul">
<li>Frame it as a classification problem</li>
<li>Classify whether a candidate answer contains an answer</li>
<li>Various features based on match to question,
expected entity type match, specific answer patterns.</li>
</ul>
</div>
</div>
<div id="outline-container-orgfc981ed" class="outline-3">
<h3 id="orgfc981ed"><span class="section-number-3">10.4</span> Neural Answer Extraction</h3>
<div class="outline-text-3" id="text-10-4">
<ul class="org-ul">
<li>Use a neural network to extract answer</li>
<li>AKA <b>reading comprehension</b> task</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org76a94b4" class="outline-2">
<h2 id="org76a94b4"><span class="section-number-2">11</span> Topic Modelling</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Isitha Subasinghe</p>
<p class="date">Created: 2021-06-15 Tue 14:56</p>
</div>
</body>
</html>
