* N-gram Language Models
 - One application NLP is about explaining language.
 - We can measure "goodness" using **probabilities**
 - Language models can also be used for **generation**
 - Useful for
   - Query completion
   - Optical character recognition
   - Other generation tasks
     - Machine translation, summarisation, dialogue systems
** Probabilities: Joint to Conditional
 $P(w_1, w_2, w_3, ..., w_m) = P(w_1)*P(w_2|w_1)*P(w_3|w_1,w_2)...P(w_m|w_1,...,w_{m-1})$
*** The Markov assumption
$P(w_i|w_1,...w_{i-1}) \approx P(w_i|w_{i-n+1}...w_{i-1})$
For some small n

When n = 1, a unigram model:

$P(w_1,w_2,...w_m) = \prod_{i=1}^{m} P(w_i)$

When n = 2, a bigram model:

$P(w_1, w_2,...w_m) = \prod_{i=1}^{m} P(w_i|w_{i-1})$

When n = 3, a trigram model:

$P(w_1,w_2,...w_m) = \prod_{i=1}^{m} P(w_i|w_{i-2} w_{i-1})$

*** Maximum Likelihood Estimation
How do we calculate the probabilities? Estimate based on counts in our corpus

For unigram models:

$P(w_i) = \frac{C(w_i)}{M}$

For bigram models:

$P(w_i|w_{i-1}) = \frac{C(w_{i-1}w_{i})}{C(w_{i-1})}$

For n gram models more generally:

$P(w_i| w_{i-n+1}...w_{i-1}) = \frac{C(w_{i-n+1}...w_i)}{C(w_{i-n+1}...w_{i-1})}$

*** Book ending sentences
- Special tokens used to indicate start and end of sequences.
  <s> = start of sentence
  </s> = end of sentence


*** Several problems
- Language has long distance effects - need large n
- Resulting probabilities are too small
  - Use log probability
- What about unseen words?
  Special symbol
- Unseen n-grams
  - Need to smooth the language model


*** Smoothing
- Basic idea: give events you haven't seen before some probability
- Must be the case that P(everything) = 1
- Many different kinds of smoothing
  - Laplacian (add-one) smoothing
  - Add-k smoothing
  - Absolute discounting
  - Kneser-Ney
*** Generation
Given an intial word, draw the next word according to the probability distribution produced by the language model.
